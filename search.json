[{"path":"https://phildonovan.github.io/redshiftr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 redshiftr authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://phildonovan.github.io/redshiftr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"package maintainer. Maintainer.","code":""},{"path":"https://phildonovan.github.io/redshiftr/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ww (2024). redshiftr: Package (Title Case). R package version 0.1.0, https://phildonovan.github.io/redshiftr/.","code":"@Manual{,   title = {redshiftr: What the Package Does (Title Case)},   author = {Who wrote it},   year = {2024},   note = {R package version 0.1.0},   url = {https://phildonovan.github.io/redshiftr/}, }"},{"path":"https://phildonovan.github.io/redshiftr/index.html","id":"redshiftr","dir":"","previous_headings":"","what":"What the Package Does (Title Case)","title":"What the Package Does (Title Case)","text":"redshiftr R package designed help users interact Amazon Redshift, particularly situations direct access S3 bucket available. package continuation original redshiftTools, longer actively maintained. building excellent foundation provided redshiftTools, redshiftr introduces new features enhancements: Table Creation Without S3 Access: Unlike original package, redshiftr allows users create tables Redshift directly, without need S3 bucket. Please note, however, process considerably slower standard method recommended small tables. Spatial Table Creation (Experimental): package also offers ability create spatial tables Redshift, feature currently active development considered highly experimental. package made possible thanks original authors redshiftTools, whose work extended improved upon . contributions laid groundwork ongoing development effort.","code":""},{"path":"https://phildonovan.github.io/redshiftr/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"What the Package Does (Title Case)","text":"can install latest version redshiftr directly GitHub using following command:","code":"# Install remotes package if you haven't already #install.packages(\"remotes\")  # Install redshiftr from GitHub remotes::install_github(\"phildonovan/redshiftr\")"},{"path":"https://phildonovan.github.io/redshiftr/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"What the Package Does (Title Case)","text":"simple example demonstrating use redshift_copy_to() function create table Redshift instance. example, ’ll use iris dataset nc dataset sf package. starting, make sure connected Redshift database using DBI::dbConnect().","code":"# Load necessary libraries library(DBI) library(redshiftr) library(sf)  # Connect to your Redshift database (replace with your connection details) con <- dbConnect(RPostgres::Postgres(),                  dbname = \"your_db_name\",                  host = \"your_host\",                  port = 5439,                  user = \"your_username\",                  password = \"your_password\")  # Example 1: Copying the iris dataset to Redshift # Create a table in Redshift using the iris dataset redshift_copy_to(con, df = iris, table_name = \"iris_table\")  # Example 2: Copying the nc dataset (from sf package) to Redshift # Load the nc dataset nc <- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))  # Create a spatial table in Redshift using the nc dataset redshift_copy_to(con, df = nc, table_name = \"nc_table\")  # Disconnect from the database dbDisconnect(con)"},{"path":"https://phildonovan.github.io/redshiftr/reference/begin_transaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Begin a Database Transaction — begin_transaction","title":"Begin a Database Transaction — begin_transaction","text":"function starts transaction executing BEGIN statement.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/begin_transaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Begin a Database Transaction — begin_transaction","text":"","code":"begin_transaction(con)"},{"path":"https://phildonovan.github.io/redshiftr/reference/begin_transaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Begin a Database Transaction — begin_transaction","text":"con DBI connection object.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/calculate_char_size.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Character Size for Column — calculate_char_size","title":"Calculate Character Size for Column — calculate_char_size","text":"function calculates appropriate character size given column data frame, used determine VARCHAR size Redshift table creation.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/calculate_char_size.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Character Size for Column — calculate_char_size","text":"","code":"calculate_char_size(col)"},{"path":"https://phildonovan.github.io/redshiftr/reference/calculate_char_size.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Character Size for Column — calculate_char_size","text":"col column data frame.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/calculate_char_size.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Character Size for Column — calculate_char_size","text":"integer representing maximum character size column.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/col_to_redshift_type.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Column to Redshift Type — col_to_redshift_type","title":"Convert Column to Redshift Type — col_to_redshift_type","text":"function determines appropriate Redshift data type given column data frame.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/col_to_redshift_type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Column to Redshift Type — col_to_redshift_type","text":"","code":"col_to_redshift_type(col, compression)"},{"path":"https://phildonovan.github.io/redshiftr/reference/col_to_redshift_type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Column to Redshift Type — col_to_redshift_type","text":"col column data frame. compression logical value indicating whether compression applied.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/col_to_redshift_type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Column to Redshift Type — col_to_redshift_type","text":"character string representing Redshift data type column.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/commit_transaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Commit a Database Transaction — commit_transaction","title":"Commit a Database Transaction — commit_transaction","text":"function commits transaction executing COMMIT statement.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/commit_transaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Commit a Database Transaction — commit_transaction","text":"","code":"commit_transaction(con)"},{"path":"https://phildonovan.github.io/redshiftr/reference/commit_transaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Commit a Database Transaction — commit_transaction","text":"con DBI connection object.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/deletePrefix.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete S3 Objects with a Given Prefix — deletePrefix","title":"Delete S3 Objects with a Given Prefix — deletePrefix","text":"function deletes multiple objects S3 bucket share common prefix. retries deletion process case 500 errors.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/deletePrefix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete S3 Objects with a Given Prefix — deletePrefix","text":"","code":"deletePrefix(prefix, bucket, split_files, key, secret, session, region)"},{"path":"https://phildonovan.github.io/redshiftr/reference/deletePrefix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete S3 Objects with a Given Prefix — deletePrefix","text":"prefix prefix objects delete S3 bucket. bucket name S3 bucket. split_files Number files delete. key AWS access key authentication. secret AWS secret key authentication. session AWS session token authentication (optional). region AWS region bucket located.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/hello.html","id":null,"dir":"Reference","previous_headings":"","what":"Hello, World! — hello","title":"Hello, World! — hello","text":"Prints 'Hello, world!'.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/hello.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hello, World! — hello","text":"","code":"hello()"},{"path":"https://phildonovan.github.io/redshiftr/reference/hello.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hello, World! — hello","text":"","code":"hello() #> Error in hello(): could not find function \"hello\""},{"path":"https://phildonovan.github.io/redshiftr/reference/list_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"List Redshift tables — list_tables","title":"List Redshift tables — list_tables","text":"Lists tables available Redshift database","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/list_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Redshift tables — list_tables","text":"","code":"list_tables(db_con)"},{"path":"https://phildonovan.github.io/redshiftr/reference/list_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Redshift tables — list_tables","text":"db_con DBI connection Redshift server","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/list_tables.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Redshift tables — list_tables","text":"data.frame table names","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/queryDo.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute a SQL Query and Return Results — queryDo","title":"Execute a SQL Query and Return Results — queryDo","text":"function executes SQL query returns results.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/queryDo.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute a SQL Query and Return Results — queryDo","text":"","code":"queryDo(dbcon, query)"},{"path":"https://phildonovan.github.io/redshiftr/reference/queryDo.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute a SQL Query and Return Results — queryDo","text":"dbcon database connection object. query SQL query string executed.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/queryDo.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute a SQL Query and Return Results — queryDo","text":"data frame containing query results.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/queryStmt.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute a SQL Statement Without Returning Results — queryStmt","title":"Execute a SQL Statement Without Returning Results — queryStmt","text":"function executes SQL statement return results (e.g., CREATE, INSERT).","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/queryStmt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute a SQL Statement Without Returning Results — queryStmt","text":"","code":"queryStmt(dbcon, query)"},{"path":"https://phildonovan.github.io/redshiftr/reference/queryStmt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute a SQL Statement Without Returning Results — queryStmt","text":"dbcon database connection object. query SQL statement string executed.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/redshift_copy_to.html","id":null,"dir":"Reference","previous_headings":"","what":"Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to","title":"Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to","text":"function copies data frame Redshift table, generating CREATE TABLE SQL inserting data chunks. chunk inserted within transaction, optional overarching transaction can controlled rollback_on_failure. rollback_on_failure set TRUE, function attempt insert chunks one overarching transaction, rolling back entirely case errors. function creates table Redshift inserts data . S3 credentials available, uses COPY command faster performance; otherwise, falls back INSERT statement.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/redshift_copy_to.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to","text":"","code":"redshift_copy_to(   df,   dbcon,   table_name = deparse(substitute(df)),   split_files,   bucket = Sys.getenv(\"AWS_BUCKET_NAME\"),   region = Sys.getenv(\"AWS_DEFAULT_REGION\"),   access_key = Sys.getenv(\"AWS_ACCESS_KEY_ID\"),   secret_key = Sys.getenv(\"AWS_SECRET_ACCESS_KEY\"),   session_token = Sys.getenv(\"AWS_SESSION_TOKEN\"),   iam_role_arn = Sys.getenv(\"AWS_IAM_ROLE_ARN\"),   wlm_slots = 1,   sortkeys,   sortkey_style = \"compound\",   distkey,   distkey_style = \"even\",   compression = T,   additional_params = \"\" )  redshift_copy_to(   df,   dbcon,   table_name = deparse(substitute(df)),   split_files,   bucket = Sys.getenv(\"AWS_BUCKET_NAME\"),   region = Sys.getenv(\"AWS_DEFAULT_REGION\"),   access_key = Sys.getenv(\"AWS_ACCESS_KEY_ID\"),   secret_key = Sys.getenv(\"AWS_SECRET_ACCESS_KEY\"),   session_token = Sys.getenv(\"AWS_SESSION_TOKEN\"),   iam_role_arn = Sys.getenv(\"AWS_IAM_ROLE_ARN\"),   wlm_slots = 1,   sortkeys,   sortkey_style = \"compound\",   distkey,   distkey_style = \"even\",   compression = T,   additional_params = \"\" )"},{"path":"https://phildonovan.github.io/redshiftr/reference/redshift_copy_to.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to","text":"df data frame dbcon RPostgres/RJDBC connection Redshift server table_name name table create insert split_files = NULL optional parameter specify amount files split . specified look amount slices Redshift determine optimal amount. bucket name temporary bucket load data. look AWS_BUCKET_NAME environment specified. region region bucket. look AWS_DEFAULT_REGION environment specified. access_key access key permissions bucket. look AWS_ACCESS_KEY_ID environment specified. secret_key secret key permissions bucket. look AWS_SECRET_ACCESS_KEY environment specified. session_token session key permissions bucket, used instead access/secret keys specified. look AWS_SESSION_TOKEN environment specified. iam_role_arn IAM role ARN permissions bucket. look AWS_IAM_ROLE_ARN environment specified. ignoring access_key secret_key set. wlm_slots amount WLM slots use bulk load http://docs.aws.amazon.com/redshift/latest/dg/tutorial-configuring-workload-management.html sortkeys = NULL Column columns sort table sortkey_style Sortkey style, can compound interleaved http://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data-compare-sort-styles.html distkey = NULL_style Distkey style, can even , key distribution use distkey = NULL parameter. http://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html compression Add encoding columns whose compression algorithm easy guess, rest upload Redshift run ANALYZE COMPRESSION additional_params Additional params send COPY statement Redshift con DBI connection object Redshift. open connection. chunk_size initial number rows per chunk insert (default 10000). value adjusted SQL statement exceeds maximum allowed size. rollback_on_failure Logical flag indicating whether roll back entire transaction failure (default TRUE). set FALSE, individual chunks fail rolled back. use_s3 logical, whether use S3 bulk data load (default TRUE)","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/redshift_copy_to.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to","text":"NULL. function performs insertion return value.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/redshift_copy_to.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to","text":"function splits data frame smaller chunks avoid exceeding Redshift's statement size limits. chunk large insert, repeatedly split half size acceptable process fails. function uses transactions ensure data integrity insertion process. chunk data inserted within transaction, overarching transaction used rollback_on_failure set TRUE. provides flexibility handling failures. Note function may take time large data frames, chunks may need repeatedly split exceed Redshift statement size limit.","code":""},{"path":[]},{"path":"https://phildonovan.github.io/redshiftr/reference/redshiftr-package.html","id":null,"dir":"Reference","previous_headings":"","what":"redshiftr: What the Package Does (Title Case) — redshiftr-package","title":"redshiftr: What the Package Does (Title Case) — redshiftr-package","text":"(maybe one line) Use four spaces indenting paragraphs within Description.","code":""},{"path":[]},{"path":"https://phildonovan.github.io/redshiftr/reference/rollback_transaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Rollback a Database Transaction — rollback_transaction","title":"Rollback a Database Transaction — rollback_transaction","text":"function rolls back transaction executing ROLLBACK statement.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rollback_transaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rollback a Database Transaction — rollback_transaction","text":"","code":"rollback_transaction(con)"},{"path":"https://phildonovan.github.io/redshiftr/reference/rollback_transaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rollback a Database Transaction — rollback_transaction","text":"con DBI connection object.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_create_statement.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Redshift Table Statement — rs_create_statement","title":"Create Redshift Table Statement — rs_create_statement","text":"function generates SQL CREATE TABLE statement Redshift based provided data frame parameters.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_create_statement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Redshift Table Statement — rs_create_statement","text":"","code":"rs_create_statement(   df,   table_name,   sortkeys = NULL,   sortkey_style = \"compound\",   distkey = NULL,   distkey_style = \"even\",   compression = TRUE )"},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_create_statement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Redshift Table Statement — rs_create_statement","text":"df data frame use generating CREATE TABLE statement. table_name character string representing name table created. sortkeys character vector column names used sort keys (optional). sortkey_style character string specifying sort key style, either 'compound' 'interleaved' (default 'compound'). distkey character string representing distribution key column (optional). distkey_style character string specifying distribution style, either 'even' '' (default 'even'). compression logical value indicating whether compression applied (default TRUE).","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_create_statement.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Redshift Table Statement — rs_create_statement","text":"character string representing CREATE TABLE statement Redshift.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_delete_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Copy Data from S3 to Redshift — rs_delete_table","title":"Copy Data from S3 to Redshift — rs_delete_table","text":"function copies data S3 bucket Redshift table using COPY command. ","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_delete_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Copy Data from S3 to Redshift — rs_delete_table","text":"","code":"rs_delete_table(con, table_name)"},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_delete_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Copy Data from S3 to Redshift — rs_delete_table","text":"con DBI connection object Redshift. table_name name table deleted.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_delete_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Copy Data from S3 to Redshift — rs_delete_table","text":"NULL. table deleted Redshift.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_delete_table.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Copy Data from S3 to Redshift — rs_delete_table","text":"Delete table Redshift function generates executes DROP TABLE SQL statement delete table Redshift.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_insert_sql.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a single SQL insert command for multiple rows — rs_insert_sql","title":"Generate a single SQL insert command for multiple rows — rs_insert_sql","text":"function generates combined SQL INSERT statement given data frame, rows chunk inserted one dbExecute() call.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_insert_sql.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a single SQL insert command for multiple rows — rs_insert_sql","text":"","code":"rs_insert_sql(.data, table_name, con)"},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_insert_sql.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a single SQL insert command for multiple rows — rs_insert_sql","text":".data data frame generate INSERT statement. table_name name table data inserted. con DBI connection object Redshift.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/rs_insert_sql.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a single SQL insert command for multiple rows — rs_insert_sql","text":"single string containing combined SQL INSERT statement.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/show_table_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Show table information — show_table_info","title":"Show table information — show_table_info","text":"Show table information","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/show_table_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show table information — show_table_info","text":"","code":"show_table_info(db_con, table_name, schema_name = \"public\")"},{"path":"https://phildonovan.github.io/redshiftr/reference/show_table_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show table information — show_table_info","text":"db_con DBI connection Redshift server table_name table name schema_name schema name, defaults 'public'","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/show_table_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Show table information — show_table_info","text":"data.frame containing table information","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/splitDetermine.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine Optimal Split Size for Data Upload — splitDetermine","title":"Determine Optimal Split Size for Data Upload — splitDetermine","text":"function determines optimal number files split data uploading Redshift. calculates number slices Redshift adjusts split size accordingly.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/splitDetermine.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine Optimal Split Size for Data Upload — splitDetermine","text":"","code":"splitDetermine(dbcon, numRows, rowSize)"},{"path":"https://phildonovan.github.io/redshiftr/reference/splitDetermine.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine Optimal Split Size for Data Upload — splitDetermine","text":"dbcon database connection object. numRows Number rows data. rowSize Estimated size row bytes.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/splitDetermine.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine Optimal Split Size for Data Upload — splitDetermine","text":"number files split data .","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/uploadToS3.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload Data to S3 Bucket — uploadToS3","title":"Upload Data to S3 Bucket — uploadToS3","text":"function uploads data frame S3 bucket, splitting multiple files necessary. function checks bucket exists retries case errors upload.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/uploadToS3.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload Data to S3 Bucket — uploadToS3","text":"","code":"uploadToS3(data, bucket, split_files, key, secret, session, region)"},{"path":"https://phildonovan.github.io/redshiftr/reference/uploadToS3.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload Data to S3 Bucket — uploadToS3","text":"data data frame upload S3. bucket name S3 bucket. split_files Number files split data upload. key AWS access key authentication. secret AWS secret key authentication. session AWS session token authentication (optional). region AWS region bucket located.","code":""},{"path":"https://phildonovan.github.io/redshiftr/reference/uploadToS3.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload Data to S3 Bucket — uploadToS3","text":"prefix string used uploaded files, NA upload fails.","code":""}]
