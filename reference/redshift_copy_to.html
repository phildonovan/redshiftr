<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to • redshiftr</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure — redshift_copy_to"><meta name="description" content="This function copies a data frame to a Redshift table, generating the CREATE TABLE SQL
and inserting the data in chunks. Each chunk is inserted within its own transaction,
and an optional overarching transaction can be controlled with rollback_on_failure.
If rollback_on_failure is set to TRUE, the function will attempt to insert all chunks in
one overarching transaction, rolling back entirely in case of any errors.
This function creates a table in Redshift and inserts data into it. If S3 credentials are available,
it uses the COPY command for faster performance; otherwise, it falls back to an INSERT statement."><meta property="og:description" content="This function copies a data frame to a Redshift table, generating the CREATE TABLE SQL
and inserting the data in chunks. Each chunk is inserted within its own transaction,
and an optional overarching transaction can be controlled with rollback_on_failure.
If rollback_on_failure is set to TRUE, the function will attempt to insert all chunks in
one overarching transaction, rolling back entirely in case of any errors.
This function creates a table in Redshift and inserts data into it. If S3 credentials are available,
it uses the COPY command for faster performance; otherwise, it falls back to an INSERT statement."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">redshiftr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure</h1>

      <div class="d-none name"><code>redshift_copy_to.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This function copies a data frame to a Redshift table, generating the <code>CREATE TABLE</code> SQL
and inserting the data in chunks. Each chunk is inserted within its own transaction,
and an optional overarching transaction can be controlled with <code>rollback_on_failure</code>.
If <code>rollback_on_failure</code> is set to TRUE, the function will attempt to insert all chunks in
one overarching transaction, rolling back entirely in case of any errors.</p>
<p>This function creates a table in Redshift and inserts data into it. If S3 credentials are available,
it uses the COPY command for faster performance; otherwise, it falls back to an INSERT statement.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">redshift_copy_to</span><span class="op">(</span></span>
<span>  <span class="va">df</span>,</span>
<span>  <span class="va">dbcon</span>,</span>
<span>  table_name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html" class="external-link">deparse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substitute.html" class="external-link">substitute</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="va">split_files</span>,</span>
<span>  bucket <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_BUCKET_NAME"</span><span class="op">)</span>,</span>
<span>  region <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_DEFAULT_REGION"</span><span class="op">)</span>,</span>
<span>  access_key <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_ACCESS_KEY_ID"</span><span class="op">)</span>,</span>
<span>  secret_key <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_SECRET_ACCESS_KEY"</span><span class="op">)</span>,</span>
<span>  session_token <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_SESSION_TOKEN"</span><span class="op">)</span>,</span>
<span>  iam_role_arn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_IAM_ROLE_ARN"</span><span class="op">)</span>,</span>
<span>  wlm_slots <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  <span class="va">sortkeys</span>,</span>
<span>  sortkey_style <span class="op">=</span> <span class="st">"compound"</span>,</span>
<span>  <span class="va">distkey</span>,</span>
<span>  distkey_style <span class="op">=</span> <span class="st">"even"</span>,</span>
<span>  compression <span class="op">=</span> <span class="cn">T</span>,</span>
<span>  additional_params <span class="op">=</span> <span class="st">""</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu">redshift_copy_to</span><span class="op">(</span></span>
<span>  <span class="va">df</span>,</span>
<span>  <span class="va">dbcon</span>,</span>
<span>  table_name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html" class="external-link">deparse</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substitute.html" class="external-link">substitute</a></span><span class="op">(</span><span class="va">df</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="va">split_files</span>,</span>
<span>  bucket <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_BUCKET_NAME"</span><span class="op">)</span>,</span>
<span>  region <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_DEFAULT_REGION"</span><span class="op">)</span>,</span>
<span>  access_key <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_ACCESS_KEY_ID"</span><span class="op">)</span>,</span>
<span>  secret_key <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_SECRET_ACCESS_KEY"</span><span class="op">)</span>,</span>
<span>  session_token <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_SESSION_TOKEN"</span><span class="op">)</span>,</span>
<span>  iam_role_arn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"AWS_IAM_ROLE_ARN"</span><span class="op">)</span>,</span>
<span>  wlm_slots <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  <span class="va">sortkeys</span>,</span>
<span>  sortkey_style <span class="op">=</span> <span class="st">"compound"</span>,</span>
<span>  <span class="va">distkey</span>,</span>
<span>  distkey_style <span class="op">=</span> <span class="st">"even"</span>,</span>
<span>  compression <span class="op">=</span> <span class="cn">T</span>,</span>
<span>  additional_params <span class="op">=</span> <span class="st">""</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-df">df<a class="anchor" aria-label="anchor" href="#arg-df"></a></dt>
<dd><p>a data frame</p></dd>


<dt id="arg-dbcon">dbcon<a class="anchor" aria-label="anchor" href="#arg-dbcon"></a></dt>
<dd><p>an RPostgres/RJDBC connection to the Redshift server</p></dd>


<dt id="arg-table-name">table_name<a class="anchor" aria-label="anchor" href="#arg-table-name"></a></dt>
<dd><p>the name of the table to create or insert into</p></dd>


<dt id="arg-split-files">split_files<a class="anchor" aria-label="anchor" href="#arg-split-files"></a></dt>
<dd><p>= NULL optional parameter to specify amount of files to split into. If not specified will look at amount of slices in Redshift to determine an optimal amount.</p></dd>


<dt id="arg-bucket">bucket<a class="anchor" aria-label="anchor" href="#arg-bucket"></a></dt>
<dd><p>the name of the temporary bucket to load the data. Will look for AWS_BUCKET_NAME on environment if not specified.</p></dd>


<dt id="arg-region">region<a class="anchor" aria-label="anchor" href="#arg-region"></a></dt>
<dd><p>the region of the bucket. Will look for AWS_DEFAULT_REGION on environment if not specified.</p></dd>


<dt id="arg-access-key">access_key<a class="anchor" aria-label="anchor" href="#arg-access-key"></a></dt>
<dd><p>the access key with permissions for the bucket. Will look for AWS_ACCESS_KEY_ID on environment if not specified.</p></dd>


<dt id="arg-secret-key">secret_key<a class="anchor" aria-label="anchor" href="#arg-secret-key"></a></dt>
<dd><p>the secret key with permissions for the bucket. Will look for AWS_SECRET_ACCESS_KEY on environment if not specified.</p></dd>


<dt id="arg-session-token">session_token<a class="anchor" aria-label="anchor" href="#arg-session-token"></a></dt>
<dd><p>the session key with permissions for the bucket, this will be used instead of the access/secret keys if specified. Will look for AWS_SESSION_TOKEN on environment if not specified.</p></dd>


<dt id="arg-iam-role-arn">iam_role_arn<a class="anchor" aria-label="anchor" href="#arg-iam-role-arn"></a></dt>
<dd><p>an IAM role ARN with permissions for the bucket. Will look for AWS_IAM_ROLE_ARN on environment if not specified. This is ignoring access_key and secret_key if set.</p></dd>


<dt id="arg-wlm-slots">wlm_slots<a class="anchor" aria-label="anchor" href="#arg-wlm-slots"></a></dt>
<dd><p>amount of WLM slots to use for this bulk load http://docs.aws.amazon.com/redshift/latest/dg/tutorial-configuring-workload-management.html</p></dd>


<dt id="arg-sortkeys">sortkeys<a class="anchor" aria-label="anchor" href="#arg-sortkeys"></a></dt>
<dd><p>= NULL Column or columns to sort the table by</p></dd>


<dt id="arg-sortkey-style">sortkey_style<a class="anchor" aria-label="anchor" href="#arg-sortkey-style"></a></dt>
<dd><p>Sortkey style, can be compound or interleaved http://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data-compare-sort-styles.html</p></dd>


<dt id="arg-distkey">distkey<a class="anchor" aria-label="anchor" href="#arg-distkey"></a></dt>
<dd><p>= NULL_style Distkey style, can be even or all, for the key distribution use the distkey = NULL parameter. http://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html</p></dd>


<dt id="arg-compression">compression<a class="anchor" aria-label="anchor" href="#arg-compression"></a></dt>
<dd><p>Add encoding for columns whose compression algorithm is easy to guess, for the rest you should upload it to Redshift and run ANALYZE COMPRESSION</p></dd>


<dt id="arg-additional-params">additional_params<a class="anchor" aria-label="anchor" href="#arg-additional-params"></a></dt>
<dd><p>Additional params to send to the COPY statement in Redshift</p></dd>


<dt id="arg-con">con<a class="anchor" aria-label="anchor" href="#arg-con"></a></dt>
<dd><p>A DBI connection object to Redshift. Should be an open connection.</p></dd>


<dt id="arg-chunk-size">chunk_size<a class="anchor" aria-label="anchor" href="#arg-chunk-size"></a></dt>
<dd><p>The initial number of rows per chunk to insert (default is 10000). This
value will be adjusted if the SQL statement exceeds the maximum allowed size.</p></dd>


<dt id="arg-rollback-on-failure">rollback_on_failure<a class="anchor" aria-label="anchor" href="#arg-rollback-on-failure"></a></dt>
<dd><p>Logical flag indicating whether to roll back the entire transaction
on failure (default is TRUE). If set to FALSE, only individual chunks that fail will be rolled back.</p></dd>


<dt id="arg-use-s-">use_s3<a class="anchor" aria-label="anchor" href="#arg-use-s-"></a></dt>
<dd><p>logical, whether to use S3 for bulk data load (default is TRUE)</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>NULL. The function performs the insertion and does not return a value.</p>
    </div>
    <div class="section level2">
    <h2 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h2>
    <p>The function splits the data frame into smaller chunks to avoid exceeding Redshift's
statement size limits. If a chunk is too large to insert, it will be repeatedly split in half
until the size is acceptable or the process fails.</p>
<p>The function uses transactions to ensure data integrity during the insertion process.
Each chunk of data is inserted within its own transaction, and an overarching transaction
is used if <code>rollback_on_failure</code> is set to TRUE. This provides flexibility in handling failures.
Note that this function may take some time for very large data frames, as chunks may need
to be repeatedly split if they exceed the Redshift statement size limit.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>

    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by The package maintainer.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer></div>





  </body></html>

