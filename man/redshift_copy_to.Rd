% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/insert-sql.R, R/redshift-copy-to.R
\name{redshift_copy_to}
\alias{redshift_copy_to}
\title{Copy a data frame to Redshift with chunked insertion, with an optional overall rollback on failure}
\usage{
redshift_copy_to(
  df,
  dbcon,
  table_name = deparse(substitute(df)),
  split_files,
  bucket = Sys.getenv("AWS_BUCKET_NAME"),
  region = Sys.getenv("AWS_DEFAULT_REGION"),
  access_key = Sys.getenv("AWS_ACCESS_KEY_ID"),
  secret_key = Sys.getenv("AWS_SECRET_ACCESS_KEY"),
  session_token = Sys.getenv("AWS_SESSION_TOKEN"),
  iam_role_arn = Sys.getenv("AWS_IAM_ROLE_ARN"),
  wlm_slots = 1,
  sortkeys,
  sortkey_style = "compound",
  distkey,
  distkey_style = "even",
  compression = T,
  additional_params = ""
)

redshift_copy_to(
  df,
  dbcon,
  table_name = deparse(substitute(df)),
  split_files,
  bucket = Sys.getenv("AWS_BUCKET_NAME"),
  region = Sys.getenv("AWS_DEFAULT_REGION"),
  access_key = Sys.getenv("AWS_ACCESS_KEY_ID"),
  secret_key = Sys.getenv("AWS_SECRET_ACCESS_KEY"),
  session_token = Sys.getenv("AWS_SESSION_TOKEN"),
  iam_role_arn = Sys.getenv("AWS_IAM_ROLE_ARN"),
  wlm_slots = 1,
  sortkeys,
  sortkey_style = "compound",
  distkey,
  distkey_style = "even",
  compression = T,
  additional_params = ""
)
}
\arguments{
\item{df}{a data frame}

\item{dbcon}{an RPostgres/RJDBC connection to the Redshift server}

\item{table_name}{the name of the table to create or insert into}

\item{split_files}{= NULL optional parameter to specify amount of files to split into. If not specified will look at amount of slices in Redshift to determine an optimal amount.}

\item{bucket}{the name of the temporary bucket to load the data. Will look for AWS_BUCKET_NAME on environment if not specified.}

\item{region}{the region of the bucket. Will look for AWS_DEFAULT_REGION on environment if not specified.}

\item{access_key}{the access key with permissions for the bucket. Will look for AWS_ACCESS_KEY_ID on environment if not specified.}

\item{secret_key}{the secret key with permissions for the bucket. Will look for AWS_SECRET_ACCESS_KEY on environment if not specified.}

\item{session_token}{the session key with permissions for the bucket, this will be used instead of the access/secret keys if specified. Will look for AWS_SESSION_TOKEN on environment if not specified.}

\item{iam_role_arn}{an IAM role ARN with permissions for the bucket. Will look for AWS_IAM_ROLE_ARN on environment if not specified. This is ignoring access_key and secret_key if set.}

\item{wlm_slots}{amount of WLM slots to use for this bulk load http://docs.aws.amazon.com/redshift/latest/dg/tutorial-configuring-workload-management.html}

\item{sortkeys}{= NULL Column or columns to sort the table by}

\item{sortkey_style}{Sortkey style, can be compound or interleaved http://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data-compare-sort-styles.html}

\item{distkey}{= NULL_style Distkey style, can be even or all, for the key distribution use the distkey = NULL parameter. http://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html}

\item{compression}{Add encoding for columns whose compression algorithm is easy to guess, for the rest you should upload it to Redshift and run ANALYZE COMPRESSION}

\item{additional_params}{Additional params to send to the COPY statement in Redshift}

\item{con}{A DBI connection object to Redshift. Should be an open connection.}

\item{chunk_size}{The initial number of rows per chunk to insert (default is 10000). This
value will be adjusted if the SQL statement exceeds the maximum allowed size.}

\item{rollback_on_failure}{Logical flag indicating whether to roll back the entire transaction
on failure (default is TRUE). If set to FALSE, only individual chunks that fail will be rolled back.}

\item{use_s3}{logical, whether to use S3 for bulk data load (default is TRUE)}
}
\value{
NULL. The function performs the insertion and does not return a value.
}
\description{
This function copies a data frame to a Redshift table, generating the \verb{CREATE TABLE} SQL
and inserting the data in chunks. Each chunk is inserted within its own transaction,
and an optional overarching transaction can be controlled with \code{rollback_on_failure}.
If \code{rollback_on_failure} is set to TRUE, the function will attempt to insert all chunks in
one overarching transaction, rolling back entirely in case of any errors.

This function creates a table in Redshift and inserts data into it. If S3 credentials are available,
it uses the COPY command for faster performance; otherwise, it falls back to an INSERT statement.
}
\details{
The function splits the data frame into smaller chunks to avoid exceeding Redshift's
statement size limits. If a chunk is too large to insert, it will be repeatedly split in half
until the size is acceptable or the process fails.

The function uses transactions to ensure data integrity during the insertion process.
Each chunk of data is inserted within its own transaction, and an overarching transaction
is used if \code{rollback_on_failure} is set to TRUE. This provides flexibility in handling failures.
Note that this function may take some time for very large data frames, as chunks may need
to be repeatedly split if they exceed the Redshift statement size limit.
}
\examples{
\dontrun{
# Example using dbplyr's `simulate_redshift` for testing without a real Redshift connection:
library(dbplyr)
con <- simulate_redshift()
df <- data.frame(a = 1:10000, b = rnorm(10000))
redshift_copy_to(con, df, table_name = "my_table")
}
library(DBI)

a=data.frame(a=seq(1,10000), b=seq(10000,1))

\dontrun{
con <- dbConnect(RPostgres::Postgres(), dbname="dbname",
host='my-redshift-url.amazon.com', port='5439',
user='myuser', password='mypassword',sslmode='require')

redshift_copy_to(df=a, dbcon=con, table_name='testTable',
bucket="my-bucket", split_files = NULL=4)

}
}
