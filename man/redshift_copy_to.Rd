% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/redshift-copy-to.R
\name{redshift_copy_to}
\alias{redshift_copy_to}
\title{Create a table from scratch or insert data into an existing table}
\usage{
redshift_copy_to(
  df,
  dbcon,
  table_name = deparse(substitute(df)),
  split_files,
  bucket = Sys.getenv("AWS_BUCKET_NAME"),
  region = Sys.getenv("AWS_DEFAULT_REGION"),
  access_key = Sys.getenv("AWS_ACCESS_KEY_ID"),
  secret_key = Sys.getenv("AWS_SECRET_ACCESS_KEY"),
  session_token = Sys.getenv("AWS_SESSION_TOKEN"),
  iam_role_arn = Sys.getenv("AWS_IAM_ROLE_ARN"),
  wlm_slots = 1,
  sortkeys,
  sortkey_style = "compound",
  distkey,
  distkey_style = "even",
  compression = T,
  additional_params = ""
)
}
\arguments{
\item{df}{a data frame}

\item{dbcon}{an RPostgres/RJDBC connection to the Redshift server}

\item{table_name}{the name of the table to create or insert into}

\item{split_files}{= NULL optional parameter to specify amount of files to split into. If not specified will look at amount of slices in Redshift to determine an optimal amount.}

\item{bucket}{the name of the temporary bucket to load the data. Will look for AWS_BUCKET_NAME on environment if not specified.}

\item{region}{the region of the bucket. Will look for AWS_DEFAULT_REGION on environment if not specified.}

\item{access_key}{the access key with permissions for the bucket. Will look for AWS_ACCESS_KEY_ID on environment if not specified.}

\item{secret_key}{the secret key with permissions for the bucket. Will look for AWS_SECRET_ACCESS_KEY on environment if not specified.}

\item{session_token}{the session key with permissions for the bucket, this will be used instead of the access/secret keys if specified. Will look for AWS_SESSION_TOKEN on environment if not specified.}

\item{iam_role_arn}{an IAM role ARN with permissions for the bucket. Will look for AWS_IAM_ROLE_ARN on environment if not specified. This is ignoring access_key and secret_key if set.}

\item{wlm_slots}{amount of WLM slots to use for this bulk load http://docs.aws.amazon.com/redshift/latest/dg/tutorial-configuring-workload-management.html}

\item{sortkeys}{= NULL Column or columns to sort the table by}

\item{sortkey_style}{Sortkey style, can be compound or interleaved http://docs.aws.amazon.com/redshift/latest/dg/t_Sorting_data-compare-sort-styles.html}

\item{distkey}{= NULL_style Distkey style, can be even or all, for the key distribution use the distkey = NULL parameter. http://docs.aws.amazon.com/redshift/latest/dg/t_Distributing_data.html}

\item{compression}{Add encoding for columns whose compression algorithm is easy to guess, for the rest you should upload it to Redshift and run ANALYZE COMPRESSION}

\item{additional_params}{Additional params to send to the COPY statement in Redshift}

\item{use_s3}{logical, whether to use S3 for bulk data load (default is TRUE)}
}
\description{
This function creates a table in Redshift and inserts data into it. If S3 credentials are available,
it uses the COPY command for faster performance; otherwise, it falls back to an INSERT statement.
}
\examples{
library(DBI)

a=data.frame(a=seq(1,10000), b=seq(10000,1))

\dontrun{
con <- dbConnect(RPostgres::Postgres(), dbname="dbname",
host='my-redshift-url.amazon.com', port='5439',
user='myuser', password='mypassword',sslmode='require')

redshift_copy_to(df=a, dbcon=con, table_name='testTable',
bucket="my-bucket", split_files = NULL=4)

}
}
